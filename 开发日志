1。选择一个ui组件，选中监控列表内内容，返回相应数据（完成）

3。添加的新工作，加入队列，执行爬取。(完成)
    队列的设计？

    爬虫程序设计一个循环，当有新内容加入到列表中，则执行，执行完成后删除刚加入内容，完毕（完成）

    每天固定一个时段读取所有用户监控内容，并加入循环列表。维护数据。

5。完善ui
    重新设计整个页面
    需要内容
        上面菜单栏
        列表栏


    在百度地图添加详情
    更改工作标记图标。

6. 每次自动生成views，models，针对一个用户可以，但是，针对多个用户同时使用，则容易发生混乱！
所以，需要生成所有数据库内包含的内容。（需要修改代码。）
解决方案：爬虫生成数据库，所有分类数据保存到一个数据表中，只是在数据库里面包含数据库分类。
这样能使用一个models解决所有问题。
2018.2.1 0:24

这个版本，学到了自动生成models，在同一个请求函数中，交互不同的内容。下一个争取使用ajax。
致此，保存这个版本。开始一个新版本。

《《《《《《《《《《《《《《《第二版序幕-------------》》》》》》》》》》》》》》》》》
1. 使用新的数据库文件（完成）
2. 更改models（完成）
3. 完善ui，完善状态栏设计。（2018.2.1 22：56完成）

4。优化百度地图体验，在地图上显示详情页。(完成）
    并且在状态栏显示工作需求。（花费 4小时）

5。完善爬虫，添加自动ip池。（待完成）
    有机会，体验scrapy，并建立自己的爬虫框架！

6。自动规划线路。并且显示。（待完成）
    用户添加自己的常用地址。
    自动规划路线。
    建立规划信息展示窗口。
    用户的其他数据可以使用mongodb完成。使用mysql用户id，作为定位。

    用户地址，51job等账号密码。用户手机号，邮箱。



7。用户能够直接点击"x"删除监控数据。（花费 1h完成，有bug，后续修复）

8。用户能在地图上设置选定工作不监控。（重要等级：一般）（待完成）

9。多进程爬虫设计。（重要，3小时，仍然有很多bug）

10. 使用双数据库，保证数据更新是稳定性。（重要 花费30分钟搞定）

11. 添加留言板功能，为了练习开发小型社区。(花费3小时，完成基本功能，缺少针对特定用户的回复功能。)

13。 使用阿里云上线整个内容。

12。 下一步重点在异步多进程爬虫。（预计2-3天搞定）

15. 添加手动投递简历选项。（基本重要）

16. 爬取数据，有一点放一点。（很重要，减少用户等待时间。）

17。ajax，异步提交数据。